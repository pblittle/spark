// Code generated by ent, DO NOT EDIT.

package ent

import (
	"context"
	"errors"
	"fmt"
	"time"

	"entgo.io/ent/dialect"
	"entgo.io/ent/dialect/sql"
	"entgo.io/ent/dialect/sql/sqlgraph"
	"entgo.io/ent/schema/field"
	"github.com/google/uuid"
	"github.com/lightsparkdev/spark/so/ent/sparkinvoice"
	"github.com/lightsparkdev/spark/so/ent/tokentransaction"
)

// SparkInvoiceCreate is the builder for creating a SparkInvoice entity.
type SparkInvoiceCreate struct {
	config
	mutation *SparkInvoiceMutation
	hooks    []Hook
	conflict []sql.ConflictOption
}

// SetCreateTime sets the "create_time" field.
func (sic *SparkInvoiceCreate) SetCreateTime(t time.Time) *SparkInvoiceCreate {
	sic.mutation.SetCreateTime(t)
	return sic
}

// SetNillableCreateTime sets the "create_time" field if the given value is not nil.
func (sic *SparkInvoiceCreate) SetNillableCreateTime(t *time.Time) *SparkInvoiceCreate {
	if t != nil {
		sic.SetCreateTime(*t)
	}
	return sic
}

// SetUpdateTime sets the "update_time" field.
func (sic *SparkInvoiceCreate) SetUpdateTime(t time.Time) *SparkInvoiceCreate {
	sic.mutation.SetUpdateTime(t)
	return sic
}

// SetNillableUpdateTime sets the "update_time" field if the given value is not nil.
func (sic *SparkInvoiceCreate) SetNillableUpdateTime(t *time.Time) *SparkInvoiceCreate {
	if t != nil {
		sic.SetUpdateTime(*t)
	}
	return sic
}

// SetSparkInvoice sets the "spark_invoice" field.
func (sic *SparkInvoiceCreate) SetSparkInvoice(s string) *SparkInvoiceCreate {
	sic.mutation.SetSparkInvoice(s)
	return sic
}

// SetExpiryTime sets the "expiry_time" field.
func (sic *SparkInvoiceCreate) SetExpiryTime(t time.Time) *SparkInvoiceCreate {
	sic.mutation.SetExpiryTime(t)
	return sic
}

// SetNillableExpiryTime sets the "expiry_time" field if the given value is not nil.
func (sic *SparkInvoiceCreate) SetNillableExpiryTime(t *time.Time) *SparkInvoiceCreate {
	if t != nil {
		sic.SetExpiryTime(*t)
	}
	return sic
}

// SetReceiverPublicKey sets the "receiver_public_key" field.
func (sic *SparkInvoiceCreate) SetReceiverPublicKey(b []byte) *SparkInvoiceCreate {
	sic.mutation.SetReceiverPublicKey(b)
	return sic
}

// SetID sets the "id" field.
func (sic *SparkInvoiceCreate) SetID(u uuid.UUID) *SparkInvoiceCreate {
	sic.mutation.SetID(u)
	return sic
}

// SetNillableID sets the "id" field if the given value is not nil.
func (sic *SparkInvoiceCreate) SetNillableID(u *uuid.UUID) *SparkInvoiceCreate {
	if u != nil {
		sic.SetID(*u)
	}
	return sic
}

// AddTokenTransactionIDs adds the "token_transaction" edge to the TokenTransaction entity by IDs.
func (sic *SparkInvoiceCreate) AddTokenTransactionIDs(ids ...uuid.UUID) *SparkInvoiceCreate {
	sic.mutation.AddTokenTransactionIDs(ids...)
	return sic
}

// AddTokenTransaction adds the "token_transaction" edges to the TokenTransaction entity.
func (sic *SparkInvoiceCreate) AddTokenTransaction(t ...*TokenTransaction) *SparkInvoiceCreate {
	ids := make([]uuid.UUID, len(t))
	for i := range t {
		ids[i] = t[i].ID
	}
	return sic.AddTokenTransactionIDs(ids...)
}

// Mutation returns the SparkInvoiceMutation object of the builder.
func (sic *SparkInvoiceCreate) Mutation() *SparkInvoiceMutation {
	return sic.mutation
}

// Save creates the SparkInvoice in the database.
func (sic *SparkInvoiceCreate) Save(ctx context.Context) (*SparkInvoice, error) {
	sic.defaults()
	return withHooks(ctx, sic.sqlSave, sic.mutation, sic.hooks)
}

// SaveX calls Save and panics if Save returns an error.
func (sic *SparkInvoiceCreate) SaveX(ctx context.Context) *SparkInvoice {
	v, err := sic.Save(ctx)
	if err != nil {
		panic(err)
	}
	return v
}

// Exec executes the query.
func (sic *SparkInvoiceCreate) Exec(ctx context.Context) error {
	_, err := sic.Save(ctx)
	return err
}

// ExecX is like Exec, but panics if an error occurs.
func (sic *SparkInvoiceCreate) ExecX(ctx context.Context) {
	if err := sic.Exec(ctx); err != nil {
		panic(err)
	}
}

// defaults sets the default values of the builder before save.
func (sic *SparkInvoiceCreate) defaults() {
	if _, ok := sic.mutation.CreateTime(); !ok {
		v := sparkinvoice.DefaultCreateTime()
		sic.mutation.SetCreateTime(v)
	}
	if _, ok := sic.mutation.UpdateTime(); !ok {
		v := sparkinvoice.DefaultUpdateTime()
		sic.mutation.SetUpdateTime(v)
	}
	if _, ok := sic.mutation.ID(); !ok {
		v := sparkinvoice.DefaultID()
		sic.mutation.SetID(v)
	}
}

// check runs all checks and user-defined validators on the builder.
func (sic *SparkInvoiceCreate) check() error {
	if _, ok := sic.mutation.CreateTime(); !ok {
		return &ValidationError{Name: "create_time", err: errors.New(`ent: missing required field "SparkInvoice.create_time"`)}
	}
	if _, ok := sic.mutation.UpdateTime(); !ok {
		return &ValidationError{Name: "update_time", err: errors.New(`ent: missing required field "SparkInvoice.update_time"`)}
	}
	if _, ok := sic.mutation.SparkInvoice(); !ok {
		return &ValidationError{Name: "spark_invoice", err: errors.New(`ent: missing required field "SparkInvoice.spark_invoice"`)}
	}
	if v, ok := sic.mutation.SparkInvoice(); ok {
		if err := sparkinvoice.SparkInvoiceValidator(v); err != nil {
			return &ValidationError{Name: "spark_invoice", err: fmt.Errorf(`ent: validator failed for field "SparkInvoice.spark_invoice": %w`, err)}
		}
	}
	if _, ok := sic.mutation.ReceiverPublicKey(); !ok {
		return &ValidationError{Name: "receiver_public_key", err: errors.New(`ent: missing required field "SparkInvoice.receiver_public_key"`)}
	}
	return nil
}

func (sic *SparkInvoiceCreate) sqlSave(ctx context.Context) (*SparkInvoice, error) {
	if err := sic.check(); err != nil {
		return nil, err
	}
	_node, _spec := sic.createSpec()
	if err := sqlgraph.CreateNode(ctx, sic.driver, _spec); err != nil {
		if sqlgraph.IsConstraintError(err) {
			err = &ConstraintError{msg: err.Error(), wrap: err}
		}
		return nil, err
	}
	if _spec.ID.Value != nil {
		if id, ok := _spec.ID.Value.(*uuid.UUID); ok {
			_node.ID = *id
		} else if err := _node.ID.Scan(_spec.ID.Value); err != nil {
			return nil, err
		}
	}
	sic.mutation.id = &_node.ID
	sic.mutation.done = true
	return _node, nil
}

func (sic *SparkInvoiceCreate) createSpec() (*SparkInvoice, *sqlgraph.CreateSpec) {
	var (
		_node = &SparkInvoice{config: sic.config}
		_spec = sqlgraph.NewCreateSpec(sparkinvoice.Table, sqlgraph.NewFieldSpec(sparkinvoice.FieldID, field.TypeUUID))
	)
	_spec.OnConflict = sic.conflict
	if id, ok := sic.mutation.ID(); ok {
		_node.ID = id
		_spec.ID.Value = &id
	}
	if value, ok := sic.mutation.CreateTime(); ok {
		_spec.SetField(sparkinvoice.FieldCreateTime, field.TypeTime, value)
		_node.CreateTime = value
	}
	if value, ok := sic.mutation.UpdateTime(); ok {
		_spec.SetField(sparkinvoice.FieldUpdateTime, field.TypeTime, value)
		_node.UpdateTime = value
	}
	if value, ok := sic.mutation.SparkInvoice(); ok {
		_spec.SetField(sparkinvoice.FieldSparkInvoice, field.TypeString, value)
		_node.SparkInvoice = value
	}
	if value, ok := sic.mutation.ExpiryTime(); ok {
		_spec.SetField(sparkinvoice.FieldExpiryTime, field.TypeTime, value)
		_node.ExpiryTime = value
	}
	if value, ok := sic.mutation.ReceiverPublicKey(); ok {
		_spec.SetField(sparkinvoice.FieldReceiverPublicKey, field.TypeBytes, value)
		_node.ReceiverPublicKey = value
	}
	if nodes := sic.mutation.TokenTransactionIDs(); len(nodes) > 0 {
		edge := &sqlgraph.EdgeSpec{
			Rel:     sqlgraph.M2M,
			Inverse: true,
			Table:   sparkinvoice.TokenTransactionTable,
			Columns: sparkinvoice.TokenTransactionPrimaryKey,
			Bidi:    false,
			Target: &sqlgraph.EdgeTarget{
				IDSpec: sqlgraph.NewFieldSpec(tokentransaction.FieldID, field.TypeUUID),
			},
		}
		for _, k := range nodes {
			edge.Target.Nodes = append(edge.Target.Nodes, k)
		}
		_spec.Edges = append(_spec.Edges, edge)
	}
	return _node, _spec
}

// OnConflict allows configuring the `ON CONFLICT` / `ON DUPLICATE KEY` clause
// of the `INSERT` statement. For example:
//
//	client.SparkInvoice.Create().
//		SetCreateTime(v).
//		OnConflict(
//			// Update the row with the new values
//			// the was proposed for insertion.
//			sql.ResolveWithNewValues(),
//		).
//		// Override some of the fields with custom
//		// update values.
//		Update(func(u *ent.SparkInvoiceUpsert) {
//			SetCreateTime(v+v).
//		}).
//		Exec(ctx)
func (sic *SparkInvoiceCreate) OnConflict(opts ...sql.ConflictOption) *SparkInvoiceUpsertOne {
	sic.conflict = opts
	return &SparkInvoiceUpsertOne{
		create: sic,
	}
}

// OnConflictColumns calls `OnConflict` and configures the columns
// as conflict target. Using this option is equivalent to using:
//
//	client.SparkInvoice.Create().
//		OnConflict(sql.ConflictColumns(columns...)).
//		Exec(ctx)
func (sic *SparkInvoiceCreate) OnConflictColumns(columns ...string) *SparkInvoiceUpsertOne {
	sic.conflict = append(sic.conflict, sql.ConflictColumns(columns...))
	return &SparkInvoiceUpsertOne{
		create: sic,
	}
}

type (
	// SparkInvoiceUpsertOne is the builder for "upsert"-ing
	//  one SparkInvoice node.
	SparkInvoiceUpsertOne struct {
		create *SparkInvoiceCreate
	}

	// SparkInvoiceUpsert is the "OnConflict" setter.
	SparkInvoiceUpsert struct {
		*sql.UpdateSet
	}
)

// SetUpdateTime sets the "update_time" field.
func (u *SparkInvoiceUpsert) SetUpdateTime(v time.Time) *SparkInvoiceUpsert {
	u.Set(sparkinvoice.FieldUpdateTime, v)
	return u
}

// UpdateUpdateTime sets the "update_time" field to the value that was provided on create.
func (u *SparkInvoiceUpsert) UpdateUpdateTime() *SparkInvoiceUpsert {
	u.SetExcluded(sparkinvoice.FieldUpdateTime)
	return u
}

// UpdateNewValues updates the mutable fields using the new values that were set on create except the ID field.
// Using this option is equivalent to using:
//
//	client.SparkInvoice.Create().
//		OnConflict(
//			sql.ResolveWithNewValues(),
//			sql.ResolveWith(func(u *sql.UpdateSet) {
//				u.SetIgnore(sparkinvoice.FieldID)
//			}),
//		).
//		Exec(ctx)
func (u *SparkInvoiceUpsertOne) UpdateNewValues() *SparkInvoiceUpsertOne {
	u.create.conflict = append(u.create.conflict, sql.ResolveWithNewValues())
	u.create.conflict = append(u.create.conflict, sql.ResolveWith(func(s *sql.UpdateSet) {
		if _, exists := u.create.mutation.ID(); exists {
			s.SetIgnore(sparkinvoice.FieldID)
		}
		if _, exists := u.create.mutation.CreateTime(); exists {
			s.SetIgnore(sparkinvoice.FieldCreateTime)
		}
		if _, exists := u.create.mutation.SparkInvoice(); exists {
			s.SetIgnore(sparkinvoice.FieldSparkInvoice)
		}
		if _, exists := u.create.mutation.ExpiryTime(); exists {
			s.SetIgnore(sparkinvoice.FieldExpiryTime)
		}
		if _, exists := u.create.mutation.ReceiverPublicKey(); exists {
			s.SetIgnore(sparkinvoice.FieldReceiverPublicKey)
		}
	}))
	return u
}

// Ignore sets each column to itself in case of conflict.
// Using this option is equivalent to using:
//
//	client.SparkInvoice.Create().
//	    OnConflict(sql.ResolveWithIgnore()).
//	    Exec(ctx)
func (u *SparkInvoiceUpsertOne) Ignore() *SparkInvoiceUpsertOne {
	u.create.conflict = append(u.create.conflict, sql.ResolveWithIgnore())
	return u
}

// DoNothing configures the conflict_action to `DO NOTHING`.
// Supported only by SQLite and PostgreSQL.
func (u *SparkInvoiceUpsertOne) DoNothing() *SparkInvoiceUpsertOne {
	u.create.conflict = append(u.create.conflict, sql.DoNothing())
	return u
}

// Update allows overriding fields `UPDATE` values. See the SparkInvoiceCreate.OnConflict
// documentation for more info.
func (u *SparkInvoiceUpsertOne) Update(set func(*SparkInvoiceUpsert)) *SparkInvoiceUpsertOne {
	u.create.conflict = append(u.create.conflict, sql.ResolveWith(func(update *sql.UpdateSet) {
		set(&SparkInvoiceUpsert{UpdateSet: update})
	}))
	return u
}

// SetUpdateTime sets the "update_time" field.
func (u *SparkInvoiceUpsertOne) SetUpdateTime(v time.Time) *SparkInvoiceUpsertOne {
	return u.Update(func(s *SparkInvoiceUpsert) {
		s.SetUpdateTime(v)
	})
}

// UpdateUpdateTime sets the "update_time" field to the value that was provided on create.
func (u *SparkInvoiceUpsertOne) UpdateUpdateTime() *SparkInvoiceUpsertOne {
	return u.Update(func(s *SparkInvoiceUpsert) {
		s.UpdateUpdateTime()
	})
}

// Exec executes the query.
func (u *SparkInvoiceUpsertOne) Exec(ctx context.Context) error {
	if len(u.create.conflict) == 0 {
		return errors.New("ent: missing options for SparkInvoiceCreate.OnConflict")
	}
	return u.create.Exec(ctx)
}

// ExecX is like Exec, but panics if an error occurs.
func (u *SparkInvoiceUpsertOne) ExecX(ctx context.Context) {
	if err := u.create.Exec(ctx); err != nil {
		panic(err)
	}
}

// Exec executes the UPSERT query and returns the inserted/updated ID.
func (u *SparkInvoiceUpsertOne) ID(ctx context.Context) (id uuid.UUID, err error) {
	if u.create.driver.Dialect() == dialect.MySQL {
		// In case of "ON CONFLICT", there is no way to get back non-numeric ID
		// fields from the database since MySQL does not support the RETURNING clause.
		return id, errors.New("ent: SparkInvoiceUpsertOne.ID is not supported by MySQL driver. Use SparkInvoiceUpsertOne.Exec instead")
	}
	node, err := u.create.Save(ctx)
	if err != nil {
		return id, err
	}
	return node.ID, nil
}

// IDX is like ID, but panics if an error occurs.
func (u *SparkInvoiceUpsertOne) IDX(ctx context.Context) uuid.UUID {
	id, err := u.ID(ctx)
	if err != nil {
		panic(err)
	}
	return id
}

// SparkInvoiceCreateBulk is the builder for creating many SparkInvoice entities in bulk.
type SparkInvoiceCreateBulk struct {
	config
	err      error
	builders []*SparkInvoiceCreate
	conflict []sql.ConflictOption
}

// Save creates the SparkInvoice entities in the database.
func (sicb *SparkInvoiceCreateBulk) Save(ctx context.Context) ([]*SparkInvoice, error) {
	if sicb.err != nil {
		return nil, sicb.err
	}
	specs := make([]*sqlgraph.CreateSpec, len(sicb.builders))
	nodes := make([]*SparkInvoice, len(sicb.builders))
	mutators := make([]Mutator, len(sicb.builders))
	for i := range sicb.builders {
		func(i int, root context.Context) {
			builder := sicb.builders[i]
			builder.defaults()
			var mut Mutator = MutateFunc(func(ctx context.Context, m Mutation) (Value, error) {
				mutation, ok := m.(*SparkInvoiceMutation)
				if !ok {
					return nil, fmt.Errorf("unexpected mutation type %T", m)
				}
				if err := builder.check(); err != nil {
					return nil, err
				}
				builder.mutation = mutation
				var err error
				nodes[i], specs[i] = builder.createSpec()
				if i < len(mutators)-1 {
					_, err = mutators[i+1].Mutate(root, sicb.builders[i+1].mutation)
				} else {
					spec := &sqlgraph.BatchCreateSpec{Nodes: specs}
					spec.OnConflict = sicb.conflict
					// Invoke the actual operation on the latest mutation in the chain.
					if err = sqlgraph.BatchCreate(ctx, sicb.driver, spec); err != nil {
						if sqlgraph.IsConstraintError(err) {
							err = &ConstraintError{msg: err.Error(), wrap: err}
						}
					}
				}
				if err != nil {
					return nil, err
				}
				mutation.id = &nodes[i].ID
				mutation.done = true
				return nodes[i], nil
			})
			for i := len(builder.hooks) - 1; i >= 0; i-- {
				mut = builder.hooks[i](mut)
			}
			mutators[i] = mut
		}(i, ctx)
	}
	if len(mutators) > 0 {
		if _, err := mutators[0].Mutate(ctx, sicb.builders[0].mutation); err != nil {
			return nil, err
		}
	}
	return nodes, nil
}

// SaveX is like Save, but panics if an error occurs.
func (sicb *SparkInvoiceCreateBulk) SaveX(ctx context.Context) []*SparkInvoice {
	v, err := sicb.Save(ctx)
	if err != nil {
		panic(err)
	}
	return v
}

// Exec executes the query.
func (sicb *SparkInvoiceCreateBulk) Exec(ctx context.Context) error {
	_, err := sicb.Save(ctx)
	return err
}

// ExecX is like Exec, but panics if an error occurs.
func (sicb *SparkInvoiceCreateBulk) ExecX(ctx context.Context) {
	if err := sicb.Exec(ctx); err != nil {
		panic(err)
	}
}

// OnConflict allows configuring the `ON CONFLICT` / `ON DUPLICATE KEY` clause
// of the `INSERT` statement. For example:
//
//	client.SparkInvoice.CreateBulk(builders...).
//		OnConflict(
//			// Update the row with the new values
//			// the was proposed for insertion.
//			sql.ResolveWithNewValues(),
//		).
//		// Override some of the fields with custom
//		// update values.
//		Update(func(u *ent.SparkInvoiceUpsert) {
//			SetCreateTime(v+v).
//		}).
//		Exec(ctx)
func (sicb *SparkInvoiceCreateBulk) OnConflict(opts ...sql.ConflictOption) *SparkInvoiceUpsertBulk {
	sicb.conflict = opts
	return &SparkInvoiceUpsertBulk{
		create: sicb,
	}
}

// OnConflictColumns calls `OnConflict` and configures the columns
// as conflict target. Using this option is equivalent to using:
//
//	client.SparkInvoice.Create().
//		OnConflict(sql.ConflictColumns(columns...)).
//		Exec(ctx)
func (sicb *SparkInvoiceCreateBulk) OnConflictColumns(columns ...string) *SparkInvoiceUpsertBulk {
	sicb.conflict = append(sicb.conflict, sql.ConflictColumns(columns...))
	return &SparkInvoiceUpsertBulk{
		create: sicb,
	}
}

// SparkInvoiceUpsertBulk is the builder for "upsert"-ing
// a bulk of SparkInvoice nodes.
type SparkInvoiceUpsertBulk struct {
	create *SparkInvoiceCreateBulk
}

// UpdateNewValues updates the mutable fields using the new values that
// were set on create. Using this option is equivalent to using:
//
//	client.SparkInvoice.Create().
//		OnConflict(
//			sql.ResolveWithNewValues(),
//			sql.ResolveWith(func(u *sql.UpdateSet) {
//				u.SetIgnore(sparkinvoice.FieldID)
//			}),
//		).
//		Exec(ctx)
func (u *SparkInvoiceUpsertBulk) UpdateNewValues() *SparkInvoiceUpsertBulk {
	u.create.conflict = append(u.create.conflict, sql.ResolveWithNewValues())
	u.create.conflict = append(u.create.conflict, sql.ResolveWith(func(s *sql.UpdateSet) {
		for _, b := range u.create.builders {
			if _, exists := b.mutation.ID(); exists {
				s.SetIgnore(sparkinvoice.FieldID)
			}
			if _, exists := b.mutation.CreateTime(); exists {
				s.SetIgnore(sparkinvoice.FieldCreateTime)
			}
			if _, exists := b.mutation.SparkInvoice(); exists {
				s.SetIgnore(sparkinvoice.FieldSparkInvoice)
			}
			if _, exists := b.mutation.ExpiryTime(); exists {
				s.SetIgnore(sparkinvoice.FieldExpiryTime)
			}
			if _, exists := b.mutation.ReceiverPublicKey(); exists {
				s.SetIgnore(sparkinvoice.FieldReceiverPublicKey)
			}
		}
	}))
	return u
}

// Ignore sets each column to itself in case of conflict.
// Using this option is equivalent to using:
//
//	client.SparkInvoice.Create().
//		OnConflict(sql.ResolveWithIgnore()).
//		Exec(ctx)
func (u *SparkInvoiceUpsertBulk) Ignore() *SparkInvoiceUpsertBulk {
	u.create.conflict = append(u.create.conflict, sql.ResolveWithIgnore())
	return u
}

// DoNothing configures the conflict_action to `DO NOTHING`.
// Supported only by SQLite and PostgreSQL.
func (u *SparkInvoiceUpsertBulk) DoNothing() *SparkInvoiceUpsertBulk {
	u.create.conflict = append(u.create.conflict, sql.DoNothing())
	return u
}

// Update allows overriding fields `UPDATE` values. See the SparkInvoiceCreateBulk.OnConflict
// documentation for more info.
func (u *SparkInvoiceUpsertBulk) Update(set func(*SparkInvoiceUpsert)) *SparkInvoiceUpsertBulk {
	u.create.conflict = append(u.create.conflict, sql.ResolveWith(func(update *sql.UpdateSet) {
		set(&SparkInvoiceUpsert{UpdateSet: update})
	}))
	return u
}

// SetUpdateTime sets the "update_time" field.
func (u *SparkInvoiceUpsertBulk) SetUpdateTime(v time.Time) *SparkInvoiceUpsertBulk {
	return u.Update(func(s *SparkInvoiceUpsert) {
		s.SetUpdateTime(v)
	})
}

// UpdateUpdateTime sets the "update_time" field to the value that was provided on create.
func (u *SparkInvoiceUpsertBulk) UpdateUpdateTime() *SparkInvoiceUpsertBulk {
	return u.Update(func(s *SparkInvoiceUpsert) {
		s.UpdateUpdateTime()
	})
}

// Exec executes the query.
func (u *SparkInvoiceUpsertBulk) Exec(ctx context.Context) error {
	if u.create.err != nil {
		return u.create.err
	}
	for i, b := range u.create.builders {
		if len(b.conflict) != 0 {
			return fmt.Errorf("ent: OnConflict was set for builder %d. Set it on the SparkInvoiceCreateBulk instead", i)
		}
	}
	if len(u.create.conflict) == 0 {
		return errors.New("ent: missing options for SparkInvoiceCreateBulk.OnConflict")
	}
	return u.create.Exec(ctx)
}

// ExecX is like Exec, but panics if an error occurs.
func (u *SparkInvoiceUpsertBulk) ExecX(ctx context.Context) {
	if err := u.create.Exec(ctx); err != nil {
		panic(err)
	}
}
